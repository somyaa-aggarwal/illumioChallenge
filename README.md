## Overview

The Flow Log Parser is a Java-based utility that reads network flow logs, applies tagging based on a lookup table, and generates statistical reports on tag usage and port-protocol combinations. This project is part of an assessment for Illumio and is designed to process up to 10MB of flow log data efficiently.

###  Features

1. Parses flow log data in AWS VPC Flow Logs (version 2) format.
2. Maps each flow log entry to a tag using a lookup table (CSV format).
3. Counts occurrences of each tag and port-protocol combination.
4. Supports case-insensitive matching.
5. Handles malformed and missing data gracefully.
6. Includes unit, integration, and load tests.

### Assumptions

1. Only default log format (AWS VPC Flow Logs version 2) is supported.
2. The lookup table file must be in CSV format with three columns: dstport,protocol,tag.
3. The protocol lookup table file must contain two columns: protocol_number,protocol_name.
4. Log parsing is case-insensitive.
5. Log entries with missing or malformed data are skipped with a warning.
6. If a port-protocol combination does not exist in the lookup table, it is tagged as "Untagged".

### System Requirements

1. Java 11+
2. Python 3+ for generating test data
3. Bash (Linux/macOS) (for running scripts)
   
###  Project Structure
````
illumioChallenge/
├── bin/
├── lib/
├── out/
│   ├── main/java/com/flowlog/parser/
│   │   ├── FlowLogParser.class
│   │   ├── io/
│   │   ├── processor/
├── output/
│   ├── port_protocol_counts_output.txt
│   ├── tag_counts_output.txt
│   ├── test_tag_counts.txt
├── resources/
│   ├── flow_logs.txt
│   ├── lookup_table.csv
│   ├── protocol_lookup.csv
├── scripts/
│   ├── compile.sh
│   ├── run.sh
│   ├── test_unit.sh
│   ├── test_integration.sh
│   ├── test_load.sh
├── src/
│   ├── main/java/com/flowlog/parser/
│   │   ├── FlowLogParser.java
│   │   ├── io/FileReaderUtil.java
│   │   ├── io/FileWriterUtil.java
│   │   ├── processor/FlowLogProcessor.java
├── test/
│   ├── integration/
│   ├── load_testing/
│   │   ├── data_generation/
│   │   │   ├── generate_large_logs.py
│   │   │   ├── generate_large_lookup.py
│   │   │   ├── generated_flow_logs.txt
│   │   │   ├── generated_lookup_table.csv
│   │   ├── run_load_test.sh
│   ├── unit/
│   │   ├── java/com/flowlog/parser/io/FileReaderUtilTest.java
│   │   ├── java/com/flowlog/parser/io/FileWriterUtilTest.java
│   │   ├── java/com/flowlog/parser/processor/FlowLogProcessorTest.java
└── README.md
````

##  Installation and Compilation
#### Clone the Repository:
````
git clone <repo-link>
cd illumioChallenge
````
Run the following command:
````
bash scripts/compile.sh
````
#### Running the Parser:
````
bash scripts/run.sh <flow_logs_file> <lookup_table_file> <protocol_lookup_file>
````
##### Expected output: ````output/tag_counts_output.txt```` ````output/port_protocol_counts_output.txt````

## Testing Strategy

### 1. Unit Tests and Integration Tests

#### To compile unit and integration test, run (in project directory):
````
bash test_scripts/compile_tests.sh
````
#### To run Unit Tests, use:
````
bash test_scripts/run_unit_tests.sh
````
#### To run Integration Tests, use:
````
bash test_scripts/run_integration_tests.sh
````
#### To all tests together, use:
````
bash test_scripts/run_all_tests.sh
````

### 2. Load Testing 
#### Steps for Load Testing

#### 1. Generate Lookup Table and Logs:

````
python test/load_testing/data_generation/generate_large_lookup.py
````

````
python test/load_testing/data_generation/generate_large_logs.py
````
##### Note:
- Default value of number of records to be generated is 100000 in ````generate_large_logs.py````. The users can give a custom value in the terminal on running the above command.
- Default value of the mappings generated by ````generate_large_lookup.py```` is 10000.
#### 2. Run Load Test:

````
time java -cp out main.java.com.flowlog.parser.FlowLogParser test/load_testing/data_generation/generated_flow_logs.txt test/load_testing/data_generation/generated_lookup_table.csv resources/protocol_lookup.csv
````
#### Check Execution Time and Performance Metrics:
1. Execution time will be displayed in the terminal.
2. Output files will be generated in the output/ directory.

### Observations: 

| Log Size         | Workers | User Time (s) | System Time (s) | CPU Usage (%) | Total Execution Time (s) |
|-----------------|---------|--------------|----------------|--------------|-------------------------|
| 10MB (100K logs) | 1       | 0.60         | 0.07           | 153%         | 0.436                   |
| 1GB (10M logs)  | 1       | 17.79        | 0.52           | 142%         | 12.839                   |
| 3GB (30M logs)  | 1       | 59.23        | 1.57           | 151%         | 40.084                   |

### Observational Analysis: 
1. The execution time scales almost linearly with the increase in log size. The processing time for 10M logs (1GB) is ~12.8s, and for 30M logs (3GB), it increases 3x to ~40s.
This confirms that this implementation has a time complexity of O(N), where N is the number of log entries.
2. The CPU utilization is consistently above 140%, indicating that the program effectively utilizes multiple CPU cores via Java's I/O operations.
However, since only 1 worker was used, the potential for parallelization is not fully utilized.

### Time Complexity Analysis

1. **Log Parsing**: O(N), where N is the number of flow log entries.
2. **Lookup Table Search**: O(1) (HashMap provides constant-time lookup).
3. **File I/O Operations**: O(N) (Each log entry is read and processed sequentially).

  Overall, the time complexity is O(N), making the parser highly efficient.

### Further Enhancements/optimizations

1. Instead of reading the entire log file into memory, logs can be processed in chunks (e.g., 1M logs per batch) to optimize memory usage.
2. Implement multi-threading to speed up log processing and leverage multiple CPU cores.
3. Extend support to various log formats beyond AWS VPC Flow Logs.
4. Store parsed results in a relational or NoSQL database for advanced querying and analytics.
5. Adapt the parser for real-time data ingestion using frameworks like Apache Kafka or Flink.


